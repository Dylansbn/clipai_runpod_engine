import os
import platform

from faster_whisper import WhisperModel


def transcribe_gpu(video_path):
    """
    Fonction intelligente :
    - Sur Mac : utilise CPU automatiquement
    - Sur RunPod : utilise le GPU CUDA
    """

    system = platform.system().lower()
    print(f"üß† Plateforme d√©tect√©e : {system}")

    # ------------------------------
    # 1Ô∏è‚É£ Cas MAC (Aucun GPU NVIDIA)
    # ------------------------------
    if system == "darwin":
        print("‚ö†Ô∏è Aucun GPU NVIDIA ‚Üí utilisation du CPU pour Whisper")
        model = WhisperModel("small", device="cpu", compute_type="int8")
    else:
        # ------------------------------
        # 2Ô∏è‚É£ Cas LINUX + CUDA (RunPod)
        # ------------------------------
        print("‚ö° Whisper GPU activ√© (CUDA)")
        model = WhisperModel("medium", device="cuda", compute_type="float16")

    segments, _ = model.transcribe(video_path)
    results = []

    for seg in segments:
        results.append({
            "start": seg.start,
            "end": seg.end,
            "text": seg.text.strip()
        })

    return results
